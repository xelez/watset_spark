{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def argmax(items, default_value = 0.0):\n",
    "    best_arg, best_value = None, default_value\n",
    "    for arg, value in items:\n",
    "        if value > best_value:\n",
    "            best_arg, best_value = arg, value\n",
    "    return best_arg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chinise Whispers (graph clustering algorithm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_major_class(class_weights):\n",
    "    cnt = defaultdict(float)\n",
    "    for cls, weight in class_weights:\n",
    "        cnt[cls] += weight\n",
    "\n",
    "    return argmax(cnt.items(), default_value=-1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def update_classes(V, reb, classes):\n",
    "    random.shuffle(V)\n",
    "    changed = False\n",
    "\n",
    "    for u in V:\n",
    "        neighbor_classes = ( (classes[v], w) for v, w in reb[u])\n",
    "        new_class = get_major_class(neighbor_classes)\n",
    "        if new_class != classes[u]:\n",
    "            classes[u] = new_class\n",
    "            changed = True\n",
    "    \n",
    "    return changed\n",
    "\n",
    "def make_clusters_from_classes(classes):\n",
    "    clusters = defaultdict(list)\n",
    "    for u, cls in classes.items():\n",
    "        clusters[cls].append(u)\n",
    "\n",
    "    return list(clusters.values())\n",
    "\n",
    "\n",
    "def reweight_edges(reb, weights_mode='DIST_LOG'):    \n",
    "    if weights_mode == 'DIST_LOG':\n",
    "        calc_w = lambda v, w: w / math.log(len(reb[v]) + 1)\n",
    "    elif weights_mode == 'DIST_NOLOG':\n",
    "        calc_w = lambda v, w: w / len(reb[v])\n",
    "    else:\n",
    "        raise ValueError('Invalid weights_mode = {!r}: should be DIST_LOG or DIST_NOLOG'.format(weights_mode))\n",
    "    \n",
    "    new_reb = {}\n",
    "    for u in reb.keys():\n",
    "        new_reb[u] = [ (v, calc_w(v, w)) for v, w in reb[u] ]\n",
    "    return new_reb\n",
    "\n",
    "# weights_mode = DIST_LOG, DIST_NOLOG, TOP\n",
    "# reb format: reb[u] = [ (edge1_v, edge1_weight), ... , (edgeN_v, edgeN_weight) ]\n",
    "def chinise_whispers(reb, max_steps = 10, weights_mode='TOP'):\n",
    "    if len(reb) == 1: #optimization shortcut\n",
    "        return [list(reb.keys())]\n",
    "    \n",
    "    if weights_mode != 'TOP':\n",
    "        reb = reweight_edges(reb, weights_mode)\n",
    "    \n",
    "    V = list(reb.keys())\n",
    "    classes = { u : i for i, u in enumerate(V) }\n",
    "    \n",
    "    for i in range(max_steps):\n",
    "        if not update_classes(V, reb, classes):\n",
    "            break\n",
    "    \n",
    "    return make_clusters_from_classes(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel chinise whispers on DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parallel_update_classes(classes_df):\n",
    "    classes_df.createOrReplaceTempView('classes')\n",
    "    \n",
    "    neighbor_classes = spark.sql(\"select e.u, c.cls, e.weight FROM edges e, classes c WHERE e.v=c.v\")\n",
    "    neighbor_classes = neighbor_classes.rdd.map(lambda row: (row[0], (row[1], row[2]))).groupByKey()\n",
    "    \n",
    "    return neighbor_classes.map(lambda pair: Row(v=pair[0], cls=get_major_class(pair[1]))).toDF()\n",
    "\n",
    "def parallel_reweight_edges(edges_df, weights_mode='DIST_LOG'):\n",
    "    edges_df.createOrReplaceTempView('edges')\n",
    "    node_degrees = spark.sql('SELECT u as v, count(v) as cnt FROM edges GROUP BY u')\n",
    "    node_degrees.createOrReplaceTempView('degrees')\n",
    "    \n",
    "    if weights_mode == 'DIST_LOG':\n",
    "        return spark.sql('SELECT e.u, e.v, (e.weight / log(d.cnt + 1)) as weight FROM edges e, degrees d WHERE e.v = d.v')\n",
    "    elif weights_mode == 'DIST_NOLOG':\n",
    "        return spark.sql('SELECT e.u, e.v, (e.weight / d.cnt) as weight FROM edges e, degrees d WHERE e.v = d.v')\n",
    "    else:\n",
    "        raise ValueError('Invalid weights_mode = {!r}: should be DIST_LOG or DIST_NOLOG'.format(weights_mode))\n",
    "\n",
    "# edges_df must be Dataframe of edges with colums: u, v, weight\n",
    "# classes_df must be Dataframe of initial classes with columns: v, cls\n",
    "# additionally sorts resulting clusters by size from larger to smaller ones\n",
    "def parallel_chinise_whispers(edges_df, classes_df, max_steps = 10, weights_mode='TOP'):\n",
    "    if weights_mode != 'TOP':\n",
    "        edges_df = parallel_reweight_edges(edges_df, weights_mode)\n",
    "    \n",
    "    edges_df.createOrReplaceTempView('edges')\n",
    "    # One possible optimization, but needs more investigation\n",
    "    ## edges_df = spark.sql('select * FROM edges CLUSTER BY edges.v')\n",
    "    ## edges_df.createOrReplaceTempView('edges')\n",
    "    \n",
    "    for i in range(max_steps):\n",
    "        classes_df = parallel_update_classes(classes_df).cache()\n",
    "    \n",
    "    classes_df.createOrReplaceTempView('classes')\n",
    "    return spark.sql(\"SELECT collect_list(c.v) as cluster FROM classes c GROUP BY c.cls ORDER BY size(cluster) DESC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph reading and writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_sense(id_to_word, s):\n",
    "    return id_to_word[s // MAX_SENSES_PER_WORD] + str(s % MAX_SENSES_PER_WORD)\n",
    "\n",
    "def format_cluster(id_to_word, cluster):\n",
    "    return '{0}\\t{1}'.format(len(cluster), ', '.join(format_sense(id_to_word, s) for s in cluster) )\n",
    "\n",
    "def format_word(id_to_word, s):\n",
    "    return id_to_word[s // MAX_SENSES_PER_WORD]\n",
    "\n",
    "def format_synset(id_to_word, cluster):\n",
    "    return ', '.join(format_word(id_to_word, s) for s in cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_graph_from_spark(path):\n",
    "    inp = sc.textFile(path).map(lambda t : t.split('\\t')).cache()\n",
    "    vertices = inp.flatMap(lambda t: t[:2]).distinct()\n",
    "    \n",
    "    # make word -> word_id and backwards mappings\n",
    "    indexed_vertices = vertices.zipWithIndex().cache()\n",
    "    word_to_id = indexed_vertices.collectAsMap()\n",
    "    id_to_word = indexed_vertices.map(lambda x : (x[1], x[0])).collectAsMap()\n",
    "    \n",
    "    # map edges from (str)\n",
    "    edges_rdd = inp.map(lambda edge: (word_to_id[edge[0]], (word_to_id[edge[1]], float(edge[2])) ) )\n",
    "    edges = edges_rdd.groupByKey().collectAsMap()\n",
    "    \n",
    "    return edges, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_synsets_to_file(id_to_word, clusters_rdd, path):\n",
    "    def format_pair(pair):\n",
    "        cluster, id = pair\n",
    "        return '{0}\\t{1}\\t{2}'.format(id, len(cluster), format_synset(id_to_word, cluster))\n",
    "    \n",
    "    synsets = clusters_rdd.zipWithIndex().map(format_pair)\n",
    "    synsets.saveAsTextFile(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making EGO graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# diffrence from original: no limit on edges count in EGO subgraph\n",
    "\n",
    "def filter_edges(edges, V):\n",
    "    return list(filter(lambda edge: edge[0] in V, edges))\n",
    "\n",
    "def make_ego(graph_edges, u):\n",
    "    V = set(map(lambda x: x[0], graph_edges[u]))\n",
    "    return { v : filter_edges(graph_edges[v], V) for v in V}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting clustered EGO-graphs to contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This one is important constant, must be big enough\n",
    "MAX_SENSES_PER_WORD = 1000\n",
    "\n",
    "def make_contexts(graph_edges, pair):\n",
    "    u, clusters = pair\n",
    "    weights = dict(graph_edges[u])\n",
    "    \n",
    "    for i, cluster in enumerate(clusters):\n",
    "        s = u*MAX_SENSES_PER_WORD + i\n",
    "        context = {v : weights[v] for v in cluster}\n",
    "        yield u, (s, context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disambiguate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cos_sim(ctx1, ctx2):\n",
    "    dot_product = 0\n",
    "    \n",
    "    sum1 = 0\n",
    "    sum2 = 0\n",
    "    \n",
    "    for u, weight in ctx1.items():\n",
    "        dot_product += weight * ctx2.get(u, 0)\n",
    "        sum1 += weight**2\n",
    "        \n",
    "    for _, weight in ctx2.items():\n",
    "        sum2 += weight**2\n",
    "        \n",
    "    return dot_product / math.sqrt(sum1 * sum2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def disambiguate_word(word_senses, context):\n",
    "    similarities = ( (sense_id, cos_sim(context, sense_context)) for sense_id, sense_context in word_senses )\n",
    "    return argmax(similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def disambiguate_context(all_senses, u, s, current_context):\n",
    "    context = dict(current_context)\n",
    "    context[u] = 1 # hack to add current word to context as in disambiguate.py\n",
    "      \n",
    "    new_context = [ (disambiguate_word(all_senses[v], context), weight) for v, weight in current_context.items() ]\n",
    "    return s, new_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def watset(input_path, output_path, local_cw_params = {}, global_cw_params = {}, use_parallel_cw = False):\n",
    "    # read input graph, edges[u] = [(edge1_v, edge1_weight), ..., (edgeN_v, edgeN_weight) ]\n",
    "    edges, word_to_id, id_to_word = read_graph_from_spark(input_path)\n",
    "    edges = sc.broadcast(edges)\n",
    "    \n",
    "    # make ego graphs and cluster them\n",
    "    ego_graphs = sc.parallelize(edges.value.keys()).map(lambda v: (v, make_ego(edges.value, v)))\n",
    "    clustered_ego_graphs = ego_graphs.map(lambda pair: (pair[0], chinise_whispers(pair[1], **local_cw_params)))\n",
    "\n",
    "    # make contexts from clustered graphs (ctx(s) in the paper)\n",
    "    # senses[s] = ctx(s) = { u_i : weight_i, ... }\n",
    "    contexts = clustered_ego_graphs.flatMap(lambda pair : make_contexts(edges.value, pair)).cache()\n",
    "    senses = contexts.groupByKey().collectAsMap()\n",
    "    senses = sc.broadcast(senses)\n",
    "\n",
    "    # Disambiguate contexts. Resulting graph is graph of word senses. \n",
    "    sense_graph_edges = contexts.map(lambda pair: disambiguate_context(senses.value, pair[0], pair[1][0], pair[1][1])).cache()\n",
    "\n",
    "    # And cluster them\n",
    "    if use_parallel_cw:\n",
    "        edges_df = sense_graph_edges.flatMap(lambda pair: (Row(u=pair[0], v=v, weight=weight) for v, weight in pair[1])).toDF().cache()\n",
    "        classes_df = sense_graph_edges.keys().map(lambda v: Row(v=v, cls=v)).toDF().cache()\n",
    "        clusters_df = parallel_chinise_whispers(edges_df, classes_df, **global_cw_params)\n",
    "        clusters_rdd = clusters_df.rdd.map(lambda x: x['cluster'])\n",
    "    else:\n",
    "        sense_graph = sense_graph_edges.collectAsMap()\n",
    "        clusters = chinise_whispers(sense_graph, **global_cw_params)\n",
    "        clusters.sort(key=len, reverse=True)\n",
    "        clusters_rdd = sc.parallelize(clusters)\n",
    "\n",
    "    save_synsets_to_file(id_to_word, clusters_rdd, output_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf test.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "watset('watset/data/ru/edges.count.txt', 'test.txt',\n",
    "       {'max_steps' : 8, 'weights_mode' : 'DIST_LOG'},\n",
    "       {'max_steps' : 10, 'weights_mode' : 'DIST_NOLOG'}, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -rf test2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "watset('watset/data/ru/edges.count.txt', 'test2.txt',\n",
    "       {'max_steps' : 8, 'weights_mode' : 'DIST_NOLOG'},\n",
    "       {'max_steps' : 10, 'weights_mode' : 'DIST_LOG'}, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "watset('watset/data/ru/edges.count.txt', 'test3.txt',\n",
    "       {'max_steps' : 8, 'weights_mode' : 'TOP'},\n",
    "       {'max_steps' : 10, 'weights_mode' : 'TOP'}, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import argparse\n",
    "\n",
    "weighting_methods = ['TOP', 'DIST_LOG', 'DIST_NOLOG']\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('input_file', help='path to input file')\n",
    "parser.add_argument('output_dir', help='path to output directory')\n",
    "parser.add_argument('--local_cw_maxsteps', type=int, default=10)\n",
    "parser.add_argument('--local_cw_weighting', choices=weighting_methods, default='TOP')\n",
    "parser.add_argument('--global_cw_maxsteps', type=int, default=10)\n",
    "parser.add_argument('--global_cw_weighting', choices=weighting_methods, default='TOP')\n",
    "parser.add_argument('--no_parallel_cw', action='store_true', default=False,\n",
    "                    help = \"Use pure python Chinise Whispers instead of pyspark implementation, probably works faster\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Watset\") \\\n",
    "        .getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    \n",
    "    watset(args.input_file, args.output_dir,\n",
    "           local_cw_params={'max_steps' : args.local_cw_maxsteps, 'weights_mode' : args.local_cw_weighting},\n",
    "           global_cw_params={'max_steps' : args.global_cw_maxsteps, 'weights_mode' : args.global_cw_weighting},\n",
    "           use_parallel_cw=(not arg.no_parallel_cw))\n",
    "    \n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
